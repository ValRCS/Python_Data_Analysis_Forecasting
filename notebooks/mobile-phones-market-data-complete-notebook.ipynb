{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Mobile Phones Market Data\n\n<img src=\"https://storage.googleapis.com/kaggle-datasets-images/1137520/1908197/93084e667e82983099e2b7611faa9407/dataset-cover.png?t=2021-02-04-08-14-14\" style=\"align:center\">\n\n<br>\n\n# Introduction\n\nThis notebook uses the [Mobile Phones Market Data](https://www.kaggle.com/artempozdniakov/ukrainian-market-mobile-phones-data) - *Data with prices and parameters of smartphones, which can be bought in Ukraine.* dataset given by [Artem Pozdniakov](https://www.kaggle.com/artempozdniakov).\n\nThe objective of this notebook is to accomplish the following tasks:\n- **Predict Prices** \n- **Exploratory Data Analysis**\n\n> *The dataset set contains data about the mobile phones which were released in past 4 years and which can be bought in Ukraine. Dataset contains the model name, brand name and operating system of the phone and it's popularity. It also has it's financial characteristics like lowest/highest/best price and sellers amount. And some of the characteristics like screen/battery size, memory amount and release date. This data can be useful for improving your machine learning, analysis and vizualization, missing data filling skills. I'm waiting for your notebooks! :) Good luck!* - **@artempozdniakov**","metadata":{}},{"cell_type":"markdown","source":"# Table of contents\n\n- Imports\n- Load the data\n- Basic insights\n- EDA | Exploratory Data Analysis\n  1. Numerical variables\n  2. Categorical variables\n  3. Time-Series\n- Modelling\n  1. Pre-Processing\n  2. Model\n- Post-Modelling\n  1. Feature importance\n  2. Predictions analysis\n- Conclusion","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns # plotting\nimport matplotlib.pyplot as plt # plot handling\nimport time # timer and stuff\nimport warnings # warning handling\n\n# Kaggle file system steup\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"markdown","source":"# Load the data","metadata":{}},{"cell_type":"code","source":"# Load the csv file\nmobiles = pd.read_csv('../input/ukrainian-market-mobile-phones-data/phones_data.csv', index_col=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dimensions","metadata":{}},{"cell_type":"code","source":"print(f\"There are {mobiles.shape[0]} rows and {mobiles.shape[1]} columns.\")\nprint(f\"There are {mobiles.isna().sum().sum()} missing values which represents {round((mobiles.isna().sum().sum() / (mobiles.shape[0] * mobiles.shape[1])) * 100, 2)}% of the data.\")\nprint(f\"Columns : {mobiles.columns.tolist()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"markdown","source":"# Basic insights","metadata":{}},{"cell_type":"code","source":"mobiles.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> First 5 rows of the DataFrame.","metadata":{}},{"cell_type":"code","source":"mobiles.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Some descriptive stats of the numerical variables.","metadata":{}},{"cell_type":"code","source":"mobiles.describe(include=['object'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Some descriptive stats of the categorical variables.","metadata":{}},{"cell_type":"code","source":"((mobiles.isna().sum()[mobiles.isna().sum()  > 0 ] / mobiles.shape[0] * 100).apply(lambda x: round(x, 1))).astype(str) + '%'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> This is the percentage of missing data by columns/variables.","metadata":{}},{"cell_type":"markdown","source":"## Data Cleaning","metadata":{}},{"cell_type":"code","source":"# Dropping the columns that I can't handle\nmobiles_names = mobiles['model_name']\nmobiles       = mobiles.drop(columns=['model_name'])\n\n# Convert release_date to datetime type\nmobiles['release_date'] = pd.to_datetime(mobiles['release_date'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mobiles.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"markdown","source":"# EDA a.k.a Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# Extract columns that are neither object nor datetime\nnumericals   = mobiles.dtypes[(mobiles.dtypes!='O') & (mobiles.dtypes!='<M8[ns]')].index.tolist()\n\n# Extract categorical variables which are objects here\ncategoricals = mobiles.dtypes[mobiles.dtypes == 'O'].index.tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constats for the EDA plots\nWIDTH  = 20\nHEIGHT = 8","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Numerical variables","metadata":{}},{"cell_type":"markdown","source":"### Functions","metadata":{}},{"cell_type":"code","source":"def plot_numerical(frame, column, categorical=None, ax=None, n_row=None, n_col=None):\n    # Simple\n    if categorical is None:\n        sns.histplot(data=frame, x=column, ax=ax[n_row][n_col])\n    \n    # With category\n    else:\n        sns.histplot(data=frame, x=column, hue=categorical, ax=ax[n_row][n_col], legend=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analysis","metadata":{}},{"cell_type":"markdown","source":"#### Distribution","metadata":{}},{"cell_type":"code","source":"n_row = -1\n\n# Setup a grid of (no. of rows, no. of plots on a row) with figure size\nfig, ax = plt.subplots(len(numericals), 1 + len(categoricals), figsize=(WIDTH, HEIGHT * (2 + len(categoricals))))\n\n# Plot the figure for numericals\nfor numerical in numericals:\n    # Increment\n    n_col = 0\n    n_row += 1\n    \n    # Single distribution plotting\n    plot_numerical(mobiles, numerical, categorical=None, ax=ax, n_row=n_row, n_col=n_col)\n    n_col += 1\n    \n    # Distribution plotting by category\n    for categorical in categoricals:\n        plot_numerical(mobiles, numerical, categorical=categorical, ax=ax, n_row=n_row, n_col=n_col)\n        n_col += 1\n\n# Display the plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot pairing","metadata":{}},{"cell_type":"markdown","source":"#### By os","metadata":{}},{"cell_type":"code","source":"by_col = 'os'\n\nsns.pairplot(mobiles[numericals + [by_col]], hue=by_col)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### By brand","metadata":{}},{"cell_type":"code","source":"by_col = 'brand_name'\n\nsns.pairplot(mobiles[numericals + [by_col]], hue=by_col)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Categorical variables","metadata":{}},{"cell_type":"markdown","source":"### Functions","metadata":{}},{"cell_type":"code","source":"def plot_categorical(frame, column):\n    # Count plot\n    sns.countplot(frame[column])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analysis","metadata":{}},{"cell_type":"code","source":"titles = ['Brand', 'Operating System']\n\nfor categorical, title in zip(categoricals, titles):\n    plt.figure(figsize=(WIDTH, HEIGHT))\n    plot_categorical(mobiles, categorical)\n    plt.title(title)\n    \n    if title == 'Brand':\n        plt.xticks(rotation=90)\n\n    plt.xlabel('')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Time-Series","metadata":{}},{"cell_type":"markdown","source":"### Date interval","metadata":{}},{"cell_type":"code","source":"earliest = mobiles['release_date'].min().strftime(\"%B %d, %Y\")\nlatest   = mobiles['release_date'].max().strftime(\"%B %d, %Y\")\n\nprint(f\"Release dates are between {earliest} and {latest}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prices evolution\n\n> Plotting only by os because there are too many brand names","metadata":{}},{"cell_type":"code","source":"for price in [ f\"{e}_price\" for e in ['lowest', 'highest', 'best']]:\n    plt.figure(figsize=(WIDTH, HEIGHT))\n    sns.lineplot(data=mobiles, x='release_date', y=price, hue='os')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Plotting lowest_price, highest_price and best_price","metadata":{}},{"cell_type":"markdown","source":"## Miscellaneous","metadata":{}},{"cell_type":"code","source":"def print_phone(phone):\n    space = 30\n    print(f\"{'Name'.rjust(space)} : {phone['model_name']}\")\n    print(f\"{'Price'.rjust(space)} : [{phone['lowest_price']}; {phone['highest_price']}]\")\n    print(f\"{'Popularity'.rjust(space)} : {phone['popularity']}\")\n    print(f\"{'Brand (OS)'.rjust(space)} : {phone['brand_name']} ({phone['os']})\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Top 5 priciest mobile phones","metadata":{}},{"cell_type":"code","source":"for i in range(5):\n    phone = mobiles.nlargest(5, 'highest_price').join(mobiles_names[mobiles.nlargest(5, 'highest_price').index]).iloc[i]\n    print_phone(phone)\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Top 5 cheapest mobile phones","metadata":{}},{"cell_type":"code","source":"for i in range(5):\n    phone = mobiles.nsmallest(5, 'highest_price').join(mobiles_names[mobiles.nsmallest(5, 'highest_price').index]).iloc[i]\n    print_phone(phone)\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"markdown","source":"# Modelling\n\nIn order to accomplish the task, we will select to predict **best_price** variable. In order to do that here are the details about the issue we are going to deal with...\n\n- Type of issue : **Regression**\n- Type of variables : **Numerical, Categorical and Time-Series**","metadata":{}},{"cell_type":"code","source":"data = mobiles.copy()\n\nfeatures = [\n    'brand_name',\n    'os',\n    'popularity',\n    'sellers_amount',\n    'screen_size',\n    'memory_size',\n    'battery_size',\n]\n\nnums  = ['popularity', 'sellers_amount', 'screen_size', 'memory_size', 'battery_size']\ncats  = ['brand_name', 'os']\n\nTARGET = 'best_price'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax= plt.subplots(1, 2, figsize=(WIDTH, HEIGHT/2))\n\nsns.histplot(mobiles[TARGET], ax=ax[0])\nax[0].title.set_text(f'{TARGET} distribution')\n\nsns.histplot(mobiles[TARGET].apply(np.log), ax=ax[1])\nax[1].title.set_text(f'Log scaled {TARGET} distribution')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Because the data looks more *Gaussian* in log scale we will predict the log scale of the target variable then we will put it to exponential for the real predictions.","metadata":{}},{"cell_type":"code","source":"data[TARGET] = data[TARGET].apply(np.log)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Pre-Processing","metadata":{}},{"cell_type":"markdown","source":"### Date variable\nWe convert datetime type to timestamp (float) for the modelling part. So the bigger the timestamp, the earlier the phone was released.\nWe won't take into account the date in this notebook version...","metadata":{}},{"cell_type":"code","source":"# data['release_date'] = data['release_date'].apply( lambda x: x.timestamp())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Numerical variables - Scaling/Normalizing\n\nIn this part you can chose either to scale or normalize your data. It might sometimes help the model to get better results, though it is a hypothesis.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Init. the scaler\nscaler = StandardScaler()\n\n# Fitting the scaler to the data\nscaled_data = scaler.fit_transform(data[nums])\ndata[nums]  = pd.DataFrame(columns=nums, data=scaled_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Numerical & Categorical variables - Imputation\n- Fill missing categorical values with `'Unknown'`.\n- In our case we will impute the missing numericals values with the median grouped by the `brand_name` and `os`.","metadata":{}},{"cell_type":"code","source":"data[cats] = data[cats].fillna('Unknown')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fill_data  = data.groupby(cats, sort=False)[nums].apply(lambda x: x.ffill().bfill())\n\ndata.loc[fill_data.index, nums] = fill_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"There are {data[nums].isna().sum().sum()} missing values which represents {round((data[nums].isna().sum().sum() / (data[nums].shape[0] * data[nums].shape[1])) * 100, 2)}% of the data.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Because there is still missing data because the aggregation could not be done on all data, we will input the rest with the median.","metadata":{}},{"cell_type":"code","source":"data[nums] = data[nums].fillna(data[nums].median())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"There are {data[nums].isna().sum().sum()} missing values which represents {round((data[nums].isna().sum().sum() / (data[nums].shape[0] * data[nums].shape[1])) * 100, 2)}% of the data.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categorical variables - One-hot enconding","metadata":{}},{"cell_type":"code","source":"# One-hot encoding\noh_cats = pd.get_dummies(data[cats])\n\n# Concatenate the on-hot encoded categorial variables to the data frame\ndata = pd.concat([\n    data.drop(columns=cats),\n    oh_cats\n], axis=1)\n\n# Correct features\nfor cat in cats:\n    if cat in features:\n        features.remove(cat)\n        \nfeatures = features + oh_cats.columns.tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Model","metadata":{}},{"cell_type":"markdown","source":"### Modelling function ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\n\nSEED  = 42\n\ndef modelling(X, y, model, f_importance=False, fit=False):\n    # Type of modelling : Train & Test basic splitting\n    importance, tt_train_score, tt_test_score  = train_test_model(X, y, model, f_importance=f_importance)\n    \n    # Type of modelling : KFold Train & Test splitting\n    kf_train_score, kf_test_score = kfold_model(X, y, model)\n    \n    if fit:\n        model.fit(X, y)\n        return model, tt_test_score, kf_test_score\n    \n    return (importance, tt_train_score, tt_test_score, kf_train_score, kf_test_score) if f_importance else (tt_train_score, tt_test_score, kf_train_score, kf_test_score)\n\ndef train_test_model(X, y, model, f_importance=True):\n    \n    importance = None\n    \n    # Train & test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=SEED)\n    \n    # Fitting\n    model.fit(X_train, y_train)\n    \n    # Scores\n    train_pred = model.predict(X_train)\n    test_pred  = model.predict(X_test)\n    \n    train_score = mean_squared_error(y_train, model.predict(X_train))\n    test_score = mean_squared_error(y_test, model.predict(X_test))\n    \n    # Feature importances\n    if f_importance:\n        try:\n            try:\n                importance = model.feature_importances_\n            except:\n                try:\n                    importance = model.coef_\n                except:\n                    pass\n            \n            features   = X.columns.tolist()\n            importance = pd.Series(index=features, data=importance)\n            return importance, train_score, test_score\n        except:\n            pass\n        \n    # Model, RMSE on train, RMSE on test\n    return importance, train_score, test_score\n\ndef kfold_model(X, y, model):\n    # Parameters & variables\n    K            = 5\n    kf           = KFold(K)\n    train_scores = list() \n    test_scores  = list() \n    \n    # Looping over the folds\n    for train_index, test_index in kf.split(X):\n        \n        # Define datasets\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        \n        # Fitting\n        model.fit(X_train, y_train)\n        \n        # Scores\n        train_pred = model.predict(X_train)\n        test_pred  = model.predict(X_test)\n        \n        train_score = mean_squared_error(y_train, model.predict(X_train))\n        test_score = mean_squared_error(y_test, model.predict(X_test))\n        \n        # Increments\n        train_scores.append(train_score)\n        test_scores.append(test_score)\n    \n    kf_train_score = np.mean(train_scores)\n    kf_test_score  = np.mean(test_scores)\n    \n    return kf_train_score, kf_test_score","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classic linear regressor\nfrom sklearn.linear_model import LinearRegression, Ridge, SGDRegressor\n\n# Regressors with variable selection\nfrom sklearn.linear_model import ElasticNet, Lars, Lasso, LassoLars\n\n# Bayesian regressor\nfrom sklearn.linear_model import ARDRegression, BayesianRidge\n\n# XGBoost\nfrom xgboost import XGBRegressor\n\nmodels = [\n    LinearRegression(),\n    Ridge(),\n    SGDRegressor(),\n    ElasticNet(),\n    Lars(),\n    LassoLars(),\n    ARDRegression(),\n    BayesianRidge(),\n    XGBRegressor()\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training and evaluation","metadata":{}},{"cell_type":"code","source":"result_cols = ['name', 'basic_train', 'basic_test', 'kf_train', 'kf_test']\nimportances = dict()\n\n# Model analysis DataFrame\nmodel_analysis = pd.DataFrame(columns=result_cols)\n\n# Splitting X & y\nX = data[features]\ny = data[TARGET]\n\nfor model in models:\n    print(f\"{type(model).__name__.rjust(20)}...\", end='')\n    \n    # Function for modelling\n    importance, tt_train_score, tt_test_score, kf_train_score, kf_test_score = modelling(X, y, model, f_importance=True, fit=False)\n    \n    # Add data from modelling\n    model_analysis = model_analysis.append(\n        pd.Series(\n            index=result_cols, \n            data=np.array([\n                type(model).__name__,\n                tt_train_score,\n                tt_test_score,\n                kf_train_score,\n                kf_test_score\n            ])), \n        ignore_index=True)\n    \n    # Add data for importance analysis\n    importances[type(model).__name__] = importance\n    print(f\" ended !\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Ranking based on test RSME : ')\nprint()\nprint(model_analysis[['name', 'basic_test', 'kf_test']].sort_values(by=['basic_test'], ascending=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Ranking based on cross validation test RSME : ')\nprint()\nprint(model_analysis[['name', 'basic_test', 'kf_test']].sort_values(by=['kf_test'], ascending=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_importance(series):\n    # Sort values\n    data = series.apply(np.abs).sort_values(ascending=False)\n    \n    # Plot\n    plt.figure(figsize=(WIDTH, HEIGHT))\n    data.plot(kind='bar')\n    plt.title(data.name)\n    plt.show()\n\n_ = pd.DataFrame.from_dict(importances)[['BayesianRidge', 'SGDRegressor', 'XGBRegressor']].apply(lambda x: plot_importance(x), axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"markdown","source":"# Post-modelling analysis","metadata":{}},{"cell_type":"markdown","source":"## Interesting features\n\n- `brand_name` : The brand name has a big influence in most of the models, it can be mostly seen as minima the third most important feature among the others. Though we can see that most of the models give the ***Apple *** brand a big importance. We suppose that ***Apple *** smartphones are the most expensive so they are easy to identify.\n\n- `screen_size` & `memory_size` : If we isolate the ***Apple *** brand, we suppose that those technical characteristcs are the most important when evaluating the price of a smartphone.","metadata":{}},{"cell_type":"markdown","source":"## Predictions analysis","metadata":{}},{"cell_type":"code","source":"data[TARGET]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model init\nmodel = XGBRegressor()\n\n# Training\n_ = model.fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for brand in mobiles.brand_name.unique():\n    # Create plot\n    plt.figure(figsize=(WIDTH, HEIGHT))\n    \n    # Filter\n    query = f\"brand_name=='{brand}'\"\n    \n    # Real data\n    sns.lineplot(data=mobiles.query(query), x='release_date', y=TARGET, label=\"Real data\")\n    \n    # Predictions\n    sns.lineplot(x=mobiles.loc[X.index].query(query)['release_date'], y=np.exp(model.predict(X.iloc[mobiles.query(query).index,:])), label=\"Predictions\")\n    \n    # Display\n    plt.title(f\"Temporal evolution of {brand} smartphone prices : Real data vs predictions\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\n\nIt was a very interesting dataset to use! Though the main challenges here would be to select the most interesting features in order to predict the prices  in the future with the Time-Series variable (because I don't know yet, I am working on it).\n\nSo hope you enjoyed, don't forget to upvote, thank you.","metadata":{}}]}